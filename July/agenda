####
Next orders of business:
-Repeat in Q&A format and classification task
-Classification task: give features and model predicts animal; then ablate PCs to see if this leads to ignorance of feature (e.g. classifies horse features where "mane" is dropped as dog)
-Repeat last token work with interpolation between manifold centroids and same for word manifolds
-Try ablating all but one PC of the manifold (find the feature of that PC with neutral other features)


For translation:
I want to implement the following alteration of lastWord.py (but as its own script, without changing lastWord.py). Take same .json dog prompts, pass half into inference with "translate from english to spanish" system prompt and the other half "translate from english to german" system prompt. Make sure to mark the concepts of these separate groups of prompts as "dog into spanish" and "dog into german" to get two manifolds. Then we want to ask: How are these manifolds similar? Is there some easy translation between the two?


####
Papers: train meta-meta-SAE on same data as Nanda paper; see if you keep getting more atomic features


####
Speculative: What happens when you prompt an LLM during inference to learn a new language of your syntactical design? How are the components of this synthetic simple language represented? Or, simpler, how differently are languages represented? Is it just some orthogonal vector indicating language or does syntax mess up more?


#####
Concerns about current experiments:
-artificial labelling of concepts pre-constrain shwo we interpret representations
-the user prompt is two sentences but all prompts.json are one.
-should we pass system prompt through when we define the manifolds as well?
-since we filter prompts.json for wordToken.py, we end up with ~half the number of prompts for manifold generation as we have for lastToken.py
-how do we deal with the system/padding tokens we we generate the manifold

####
Pre-discarded ideas:
-Repeat wordToken for multi-word perturbations
